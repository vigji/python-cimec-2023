{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f54dd50c",
   "metadata": {},
   "source": [
    "# Assignments - module 1\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vigji/python-cimec/blob/main/assignments/Assignments_1.ipynb)\n",
    "\n",
    "This notebook contains the assignments to complete for credits for the first module. \n",
    "\n",
    "**Submission**: Once you're happy with your solutions, send it to me in any form (email the file, share it through Colab/Google Drive, send me a link to your GitHub repo...).\n",
    "\n",
    "**Deadline**: 15th of July 2023\n",
    "\n",
    "**Evaluation**: There is no grade, but I will pass assignments that showcase a reasonable degree of understanding og the covered topics. Do your best, and feel free to ask for help if you are struggling! \n",
    "\n",
    "(Also, try to keep in mind not only the goal of the exercise, but also all the coding best practices we have been considering in the lectures.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a6d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c41f5f",
   "metadata": {},
   "source": [
    "## 0. Spike detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0797d4be",
   "metadata": {},
   "source": [
    "In this exercise we will be playing around with some (dummy) electrophysiology recordings. Let's start by having a look at the raw data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070113ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spike_trace(trace_length=60, firing_rate=1, noise_sigma = 0.05):\n",
    "    \"\"\"Function to generate a fake extracellular recording.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        trace_length : float\n",
    "            Duration of the recording in seconds.\n",
    "        \n",
    "        firing_rate : float\n",
    "            Average firing rate of the neuron in Hz.\n",
    "            \n",
    "        noise_sigma : float\n",
    "            Noise level.\n",
    "            \n",
    "            \n",
    "    Returns:\n",
    "    --------\n",
    "        np.array\n",
    "            Fake extracellular recording. \n",
    "    \n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    FS = 10000  # sampling frequency\n",
    "    n = int(trace_length * FS)  # number of samples\n",
    "    \n",
    "    # Generate spike shape template as a difference of Gaussians.\n",
    "    # A horrible bunch of magic numbers - do not imitate!\n",
    "    x = np.arange(30)\n",
    "    spike_template = np.exp(-(x - 10)**2/6) - np.exp(-(x - 12)**2/16)*0.8\n",
    "\n",
    "    # Generate spike times from a gaussian distribution:\n",
    "    spikes_times = np.random.poisson(firing_rate / FS, n)\n",
    "    \n",
    "    # Convolve dirac delta functions of spike times with spike template:\n",
    "    trace = np.convolve(spikes_times, spike_template)[:n]\n",
    "\n",
    "    # Add some gaussian noise:\n",
    "    trace += np.random.normal(0, noise_sigma, n)\n",
    "    \n",
    "    return trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb2b279",
   "metadata": {},
   "source": [
    "### Exercise 0.0\n",
    "\n",
    "Run the function below to generate an synthetic extracellular recording for a neuron. Make a nice plot with the trace; the spikes are the peaks appearing above the noise! \n",
    "\n",
    "---\n",
    "\n",
    "(_Optional_) If you want to make a plot with exact x coordinates in seconds, you should know that the trace is sampled at 10000 Hz (10000 points per second)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b720123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb6a9b9a",
   "metadata": {},
   "source": [
    "### Exercise 0.1\n",
    "\n",
    "Write a function to detect spikes!\n",
    "The function should take the trace as input, and return the index of each spike as the output (as the index, you should take the position of the spike maximum)\n",
    "\n",
    "Hint: a good strategy to detect such events is to set a threshold, and look for elements above it. This will not be enough! each spike could have more than 1 point above the threshold, but you want to make sure you take only the spike peak! For this, you will probably need a loop.\n",
    "\n",
    "Hint: do not start from writing the function. First debug your code running it in a cell, then move it to a function.\n",
    "\n",
    "Hint: if you want, you can quickly check out the results you are getting by making a scatter plot of the detected spikes overimposed on the electrophysiology trace! (use as x of the dots the indexes of the spikes, and as y the hight of the trace at those indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8acf49c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bb3cb4d",
   "metadata": {},
   "source": [
    "### Exercise 0.2\n",
    "\n",
    "We now want to have a look at the shape of those spikes. For this, we will create a function that crops small chunks of the trace around each spike peak.\n",
    "\n",
    "Write a `crop_event()` function that takes as inputs:\n",
    "   - the recording array\n",
    "   - the spike indexes\n",
    "   - a `n_points_pad` variable specifying the number of points to include before and after the spike\n",
    "\n",
    "And returns a matrix of shape `(n_spikes, n_points*2)` containing the trace chunks cropped around spike events! \n",
    "\n",
    "Hint: A good strategy coult be initialize an empty matrix and then fill it in a loop with the trace around the spikes.\n",
    "\n",
    "**This function can be very useful in many contexts!** You can use it every time you want to crop a timeseries around events (e.g., EEG data or video kinematics data around some stimuli). So keep it at hand in the future!\n",
    "\n",
    "---\n",
    "\n",
    "(_Optional_) Pro challenge: Try to do it without for loops! if you construct a matrix with the indexes of the points you want to exctract from the trace, you can use it directly to index the trace!\n",
    "For indexing in this way, you want to build a matrix that looks like this:\n",
    "```\n",
    "array([[...t0-2, t0-1, t0, t0+1, t0+2...],\n",
    "       [...t1-2, t1-1, t1, t1+1, t1+2...],\n",
    "       [...t2-2, t2-1, t2, t2+1, t2+2...],])\n",
    "```\n",
    "Where `t0`, `t1`, `t2`... are the indexes of each spike, and you take as many points before and after as specified by the `n_points_pad` paramenter. \n",
    "\n",
    "Building this matrix without loops is not trivial but it can be done nicely with numpy broadcasting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ce81b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dcb72d6",
   "metadata": {},
   "source": [
    "### Exercise 0.3\n",
    "\n",
    "Finally, make two subplots one close to the other. On the left, use `plt.matshow` to show the spike matrix. On the right,\n",
    "plot each individual spike (rows of the matrix) using `plt.plot` with gray lines, and the average spike shape in red on top.\n",
    "\n",
    "---\n",
    "\n",
    "(Optional) If you want you can try to normalize the matrix before plotting by subtracting the average of each row (as we were doing for the daily temperatures)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e151272d",
   "metadata": {},
   "source": [
    "## Real books data\n",
    "\n",
    "After having appreciated how many books the universe of all possible books contains, let's now focus just on the reachable ones - and how much people like them! \n",
    "\n",
    "Here, we will download the information about about thousands volumns available on Amazon. Just a tiny fraction of Babel's books, but way more organized!\n",
    "\n",
    "We will also get a dataset of users writing reviews, and a dataset of reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6da12a2",
   "metadata": {},
   "source": [
    "### Exercise 0.1\n",
    "\n",
    "Using, `pandas`, read the `.csv` files containing the books, the ratings, and the user data that you can find at the  urls defined below.\n",
    "\n",
    "Hint: remember to pass the `index_col` argument to the csv reading function to handle the column of indexes in the file!\n",
    "\n",
    "Then, plot an histogram of all the ratings from all users, and another histogram with the age of the users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c155b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df_url = \"https://github.com/vigji/python-cimec/raw/main/assignments/files/users.csv\"\n",
    "ratings_df_url = \"https://github.com/vigji/python-cimec/raw/main/assignments/files/ratings.csv\"\n",
    "books_df_url = \"https://github.com/vigji/python-cimec/raw/main/assignments/files/books.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc3ac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = pd.read_csv(users_df_url, index_col=0)\n",
    "ratings_df = pd.read_csv(ratings_df_url, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4606f575",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(users_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040544e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_scat = pd.concat([ratings_df.groupby(\"user_id\")[\"rating\"].mean(), users_df.set_index(\"user_id\")], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66a4de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.scatter(rating_scat[\"age\"], rating_scat[\"rating\"], alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f73958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isbn10_ok(s):\n",
    "    data = [c for c in s if c in '0123456789Xx']\n",
    "    if len(data) != 10: return False\n",
    "    if data[-1] in 'Xx': data[-1] = 10\n",
    "    try:\n",
    "        return not sum((10 - i) * int(x) for i, x in enumerate(data)) % 11\n",
    "    except ValueError:\n",
    "        # rare case: 'X' or 'x' in first 9 \"digits\"\n",
    "        return False\n",
    "\n",
    "books = pd.read_csv(\"/Users/vigji/Downloads/books.csv\", sep=\";\", \n",
    "                    on_bad_lines=\"skip\", encoding=\"latin-1\")\n",
    "\n",
    "books = books[[\"ISBN\", \"Book-Title\", \"Book-Author\"]]\n",
    "books.columns = [\"ISBN\", \"title\", \"author\"]\n",
    "books = books[books[\"ISBN\"].apply(isbn10_ok)]\n",
    "\n",
    "books.to_csv(\"/Users/vigji/Downloads/books_clean.csv\", index=False)\n",
    "\n",
    "users = pd.read_csv(\"/Users/vigji/Downloads/users.csv\", sep=\";\", \n",
    "                    on_bad_lines=\"skip\", encoding=\"latin-1\")\n",
    "\n",
    "users.columns = [\"user_id\", \"location\", \"age\"]\n",
    "users[\"country\"] = users[\"location\"].apply(lambda x: x.split(\",\")[-1])\n",
    "\n",
    "filt = users[\"country\"].value_counts() > 200\n",
    "users = users[users[\"country\"].isin(filt[filt].index)]\n",
    "users = users.dropna()\n",
    "users = users[(users[\"age\"] < 95) & (users[\"age\"] > 10)]\n",
    "users = users[users[\"country\"].apply(lambda x: len(x) > 1)]\n",
    "users[\"country\"] = users[\"country\"].apply(lambda x: x[1:])\n",
    "users = users.drop(\"location\", axis=1)\n",
    "\n",
    "users.to_csv(\"/Users/vigji/Downloads/users.csv\", index=False)\n",
    "\n",
    "ratings = pd.read_csv(\"/Users/vigji/Downloads/ratings.csv\", sep=\";\", \n",
    "                    on_bad_lines=\"skip\", encoding=\"latin-1\")\n",
    "\n",
    "ratings.columns = [\"user_id\", \"ISBN\", \"rating\"]\n",
    "\n",
    "ratings = ratings[ratings[\"ISBN\"].isin(books[\"ISBN\"])]\n",
    "ratings = ratings[ratings[\"user_id\"].isin(users[\"user_id\"])]\n",
    "ratings = ratings[ratings[\"rating\"] > 0]\n",
    "\n",
    "val_counts = ratings[\"ISBN\"].value_counts() > 3\n",
    "ratings = ratings[ratings[\"ISBN\"].isin(val_counts[val_counts].index)]\n",
    "\n",
    "ratings.to_csv(\"/Users/vigji/Downloads/ratings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc94c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "?users.to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba8e47c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:course_env]",
   "language": "python",
   "name": "conda-env-course_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
